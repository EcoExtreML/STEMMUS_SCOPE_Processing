"""PyStemmusScope save module.

Module designed to create csv files (following SCOPE format) and a netcdf
file (following ALMA cf convention) in the output directory.

https://scope-model.readthedocs.io/en/latest/outfiles.html
https://web.lmd.jussieu.fr/~polcher/ALMA/convention_output_3.html
"""

import logging
import pandas as pd
import xarray as xr
import numpy as np

from pathlib import Path
from typing import Dict, List
from PyStemmusScope import forcing_io
from . import variable_conversion as vc

logger = logging.getLogger(__name__)



def _select_forcing_variables(forcing_dict: Dict, forcing_var: str, alma_var: str) -> xr.DataArray:
    """Select the variable needed by ALMA convention.

    Args:
        forcing_dict(dict): a dictionary returned by `PyStemmusScope.forcing_io.read_forcing_data()`.
        forcing_name(str): variable name in forcing dataset.
        alma_name(str): variable name in ALMA convention.

    Returns:
        data_array: a data array which its variable name is alma_name.
    """

    # select the forcing variable
    data_array = forcing_dict[forcing_var]

    # rename the variable name from forcing_name to alma_name
    data_array.rename({forcing_var:alma_var})
    return data_array


def _resize_data_array(data: xr.DataArray, time_steps: str)-> xr.DataArray:
    """Resize data based on time_steps.

    Args:
        data(xr.DataArray): data to be resized.
        time_steps(str): number of time steps to resize.

    Returns:
        xr.DataArray: subset of data with the lenght of time equal to time_steps.
    """

    if time_steps != "NA":
        time_length = int(time_steps)
        data = data.isel(time=np.arange(0, time_length))

    return data


def _prepare_4d_data(file_name: str, var_name: str) -> pd.DataFrame:
    """Reshape a `pandas.DataFrame` to include z dimension.

    Args:
        file_name(str): csv file name generated by Stemmus_Scope model.
        var_name(str): variable name by ALMA convention.

    Returns:
        pd.DataFrame: a dataframe with two indexes.
    """
    # the first three rows are depth, thickness
    data = pd.read_csv(file_name, delimiter=",", header=[0, 1])

    # skip first row that is unit
    data = data.iloc[1:]

    # make sure it is float and not str
    data = data.astype('float32')

    # get depth, thickness info
    depths = []
    thicknesses = []

    for depth, thickness in data.columns:
        depths.append(float(depth))
        thicknesses.append(float(thickness))

    #TODO fix bin_to_csv.m for correct Sim_Temp.csv file Sim_Theta.csv
    # drop thickness
    data = data.droplevel(level=1, axis=1)

    # change depth to indices
    data.columns = range(1, data.shape[1] + 1)


    data = data.stack()
    data.index.names = ["index", "z"]
    data.name = var_name

    if var_name == "SoilTemp":
        # Celsius to Kelvin : K = 273.15 + C
        data = data + 273.15

    elif var_name == "SoilMoist":
        thicknesses = np.array(thicknesses) / 100.0

        for index, thickness in thicknesses:
            # cm to m
            thickness = thickness / 100.0

            # m3/m3 to kg/m2
            data.loc[:, index] = vc.soil_moisture(data.loc[:, index], thicknesses)

    # soil layer metadata
    metadata = _create_soil_layer_metadata
    return data, metadata


def _prepare_3d_data(file_name: str, var_name: str) -> pd.DataFrame:
    """Reshape a `pandas.DataFrame` to include z dimension.

    Args:
        file_name(str): csv file name generated by Stemmus_Scope model.
        var_name(str): variable name by Stemmus_Scope model.

    Returns:
        pd.DataFrame: a dataframe with two indexes.
    """
    # the first three rows are names and units
    data = pd.read_csv(file_name, delimiter=",")

    # select variable and skip first row that is unit
    data = data[var_name].iloc[1:]

    # make sure it is float and not str
    data = data.astype('float32')

    return data


def _create_soil_layer_metadata(thicknesses, depths):

    """
    layer_1: 0 - 1 cm
    layer_2: 1 - 2 cm
    layer_3: 2 - 3 cm
    """

    metadata = []
    for index, (thickness, depth) in enumerate(zip(thicknesses, depths)):
        metadata.append(f"layer_{index}: {(depth - thickness)} - {depth} cm")

    return metadata


def _create_dataarray(
    data_frame: pd.DataFrame, model_var: str, alma_var: str, time: List
    ) -> xr.DataArray:
    """Reshape a `pandas.DataFrame` to include z dimension.

    Args:
        data(pd.DataFrame): path to the csv file generated by Stemmus_Scope model.
        var_name(str): variable name used by Stemmus_Scope model.

    Returns:
        pd.DataFrame: a dataframe with two indexes.
    """

    # convert dataframe to xarray data array
    data_array = data_frame.to_xarray()

    data_array["index"] = time
    data_array.rename({"index": "time"})
    data_array.rename({model_var: alma_var})
    return data_array


def _update_dataset_dims(dataset: xr.Dataset) -> xr.Dataset:
    """Update dimentions of a dataset according to ALMA conventions.

    Args:
        dataset(xr.Dataset): a dataset with varaibles in ALMA conventions.

    Returns:
        xr.Dataset: the dataset with dimensions ("time", "x", "y").
    """

    # add x/y dims to the dataset
    dataset_expanded = dataset.expand_dims(["x", "y"])

    # change the order of dims
    dataset_reordered = dataset_expanded.transpose("time", "x", "y")

    # update values of x and y coords
    dataset_reordered.assign_coords(
        {
            "x":dataset_reordered["longitude"].values.flatten(),
            "y":dataset_reordered["latitude"].values.flatten(),
            }
        )
    return dataset_reordered


def save_to_netcdf(config: Dict, cf_filename: str) -> str:
    """Save csv files generated by Stemmus_Scope model to a netcdf file using
        information provided by ALMA conventions.

    Args:
        config(Dict): PyStemmusScope configuration dictionary.
        cf_filename(str): Path to a csv file for ALMA conventions.

    Returns:
        str: path to a csv file under the output directory.
    """

    # list of required forcing variables, Alma_short_name: forcing_io_name
    # they called ECdata
    var_names = {
        "Ta": "t_air_celcius",
        "Rin": "sw_down",
        "Rli": "lw_down",
        "u": "wind_speed",
        "p": "psurf_hpa",
        "RH": "rh",
        "year": "year",
        "Pre": "precip_conv",
        "CO2air": "co2_conv",
        "Qair": "Qair",
    }

    forcing_file = Path(config["ForcingPath"]) / config["ForcingFileName"]

    # Number of time steps from configuration file
    time_steps = config["NumberOfTimeSteps"]

    # read forcing file into a dict
    forcing_dict = forcing_io.read_forcing_data(forcing_file)

    # get time info
    time = _resize_data_array(forcing_dict["time"], time_steps)

    # read convention file
    conventions = pd.read_csv(cf_filename)

    alma_short_names = conventions["short_name_alma"]
    data_list = []
    for alma_name in alma_short_names:
        df = conventions.loc[alma_short_names == alma_name].iloc[0]
        file_name = df["File name"]
        model_name = df["Variable name in STEMMUS-SCOPE"]

        if alma_name in var_names:
            # select data
            forcing_name = var_names[alma_name]
            data_array = _select_forcing_variables(config, forcing_dict, forcing_name, alma_name)
            data_array = _resize_data_array(data_array, time_steps)

        else:
            # create data array
            if alma_name in {"SoilTemp", "SoilMoist"}:
                data_frame, metadata = _prepare_4d_data(file_name, alma_name)
            else:
                data_frame = _prepare_3d_data(file_name, alma_name)

            dataarray = _create_dataarray(data_frame, model_name, alma_name, time)

        # update attributes of array
        dataarray.attrs = {
            "units": df["unit"],
            "long_name": df["long_name"],
            "standard_name": df["standard_name"],
            "STEMMUS-SCOPE_name": df["Variable name in STEMMUS-SCOPE"],
            "definition": df["definition"],
        }

        # add to list
        data_list.append(dataarray)

    # add lat/lon to the list
    data_list.extend(
        [{"latitude": forcing_dict["latitude"]},
        {"longitude": forcing_dict["longitude"]}]
    )

    # merge to a dataset
    dataset = xr.merge(data_list)

    # update time attributes
    dataset["time"].attrs.update(
        {
            "calendar": time.encoding["calendar"],
            "time_units": time.encoding["units"],
            }
            )
    # add z attributes
    dataset["z"].attrs = {
        "long_name": "Soil layer",
        "standard_name": "Soil layer",
        "definition": metadata,
        "units": "-",
    }

    # additional metadata
    dataset.attrs = {
        'model': 'STEMMUS_SCOPE',
        'institution': 'University of Twente; Northwest A&F University',
        'contact': (
            'Zhongbo Su, z.su@utwente.nl; '
            'Yijian Zeng, y.zeng@utwente.nl; '
            'Yunfei Wang, y.wang-3@utwente.nl'
            ),
        'license_type': 'CC BY 4.0',
        'license_url': 'https://creativecommons.org/licenses/by/4.0/',
        'latitude': forcing_dict["latitude"],
        'longitude': forcing_dict["longitude"]
        }

    # update dimensions
    dataset = _update_dataset_dims(dataset)

    #save to nc file
    output_dir = Path(config["OutputPath"])
    nc_filename = output_dir / f"{output_dir.stem}_STEMMUS_SCOPE.nc"
    dataset.to_netcdf(path= nc_filename, mode='w', format="NETCDF3_CLASSIC")
    return nc_filename










